# README

## 1. 项目背景与目标

随着公司需要处理和生成大量标书相关文档，为了提高效率并保证数据保密性，我们计划在本地部署大语言模型（LLM）来完成以下流程：

1. **文档输入**：将现有的标书资料、相关文件输入并储存，方便后续检索。  
2. **问题输入**：用户提出针对标书的具体问题。  
3. **文档搜索/调取**：系统会根据问题检索内部已储存的所有相关资料。  
4. **生成答案**：将“检索到的资料 + 用户问题”交由本地部署的LLM进行生成，得到完善的标书内容。

最终的目标是：**空白标书和标书相关文件作为输入 -> 完整标书作为输出**。

---

## 2. 总体设计思路

1. **数据存储与检索**  
   - 搭建一个内部文档库，能够对标书资料、合同条款、行业规范等进行结构化或非结构化的存储。  
   - 使用向量数据库或全文检索引擎（例如 ElasticSearch / Milvus / Faiss 等）对文档进行索引，方便在后续对问题进行检索。  

2. **本地LLM部署**  
   - 为满足公司内部数据的保密需求，放弃云端或托管GPU服务，转而在自研服务器上部署LLM模型。  
   - 部署的模型为 **Deepseek r1**，并采用 **Ktransformer** 模式进行推理。  

3. **流程概览**  
   1. **资料导入**：将公司内部标书相关文件导入系统，自动完成解析与索引建立。  
   2. **问题输入**：用户输入与标书相关的问题或需求。  
   3. **检索匹配**：系统对用户问题进行解析并检索最匹配的文档片段。  
   4. **回答生成**：基于检索到的文档片段以及用户问题，交由LLM进行答案生成或自动完成标书内容。  
   5. **结果输出**：生成完成的标书或解答文本。  

---

## 3. Ktransformer 模式简介

**Ktransformer** 是一种针对大语言模型的优化部署策略，能够在保持较高推理准确度的同时，显著降低硬件资源占用与推理成本。其主要特点包括：  
1. **分块推理**：将超大型模型切分为多个子模块或分层网络分块，按需调用计算资源。  
2. **自动并行与流水线**：对显存、内存进行自动化管理，实现模型在有限显存下的高效并行推理。  
3. **内存优化**：通过对注意力层与前馈层做分块处理，大幅度减少瞬时占用的显存量；同时实现更灵活的 batch 处理。  

结合 **Deepseek r1** 的优化，Ktransformer 能在较小规模的GPU和CPU配置下维持较为可观的推理速度和准确度，极大减少整体的运维和硬件成本。

---

## 4. RAG 技术与本地知识库的实现

### 4.1 RAG（Retrieval Augmented Generation）简介

**RAG**（Retrieval Augmented Generation）是一种将检索与生成相结合的技术框架。与单纯依赖大语言模型内置知识库不同，RAG会先基于向量检索或关键词检索在**外部知识库**中查找到最匹配的信息，然后将这些检索到的“外部文本”与用户问题一并输入给模型进行生成式回答，从而大幅提高答案的准确性和时效性。其主要流程如下：

1. **问题解析（Query Understanding）**：对用户的输入进行解析或向量化。  
2. **检索（Retrieval）**：在**本地文档知识库**（如 ElasticSearch / Milvus / Faiss 等）中，通过相似度匹配或关键词搜索获取最相关的信息或文档片段。  
3. **答案生成（Generation）**：将检索到的信息与用户问题合并后送入LLM中，进行上下文融合与文本生成。  
4. **后处理（Post-Processing）**：对生成内容进行结构化、摘要或自动格式化，以满足业务需求（如输出完整标书文本）。

### 4.2 如何基于 RAG 实现本地知识库

1. **文档存储与索引**  
   - 将标书相关文件（Word、PDF 等）进行文本解析并按照段落或小片段拆分，存入本地数据库或向量数据库。  
   - 对每个片段提取向量特征或进行关键词索引，形成可快速检索的索引库。  

2. **检索层搭建**  
   - 通过 REST API 或内部服务的方式，提供检索功能。  
   - 用户提交问题后，系统会将其转化为向量或关键词查询，并在知识库中进行匹配，选出得分最高的若干文档片段。  

3. **融合生成**  
   - 将检索到的文档片段与用户的问题一起作为上下文输入给本地部署的 **Deepseek r1** 模型。  
   - LLM在生成过程中会利用检索片段的关键信息，从而产出与公司内部资料更紧密结合、更加准确的回答。  

4. **更新与维护**  
   - 当新文档被添加或旧文档被修改，需要及时更新索引库，以保证检索到的内容始终是最新或最准确的。  
   - 可以定期对历史文档进行清理或归档，以提升检索效率并节约存储空间。  

结合 RAG 技术，系统即可实现对本地知识库的高效利用，既充分发挥了本地文档的丰富信息，又降低了对模型本身内置知识库的依赖；同时也强化了对私有数据的保密与保护。

---

## 5. 设备与预算规划

根据现有需求，我们初步拟定了以下硬件清单，以满足在本地部署并运行 Deepseek r1（Ktransformer模式）所需的算力和存储需求：

- **CPU**: AMD 9950X  
  - 大核数、多线程，保证编译、检索及并行数据处理时的性能需求。  
- **主板**: X870E  
  - 兼容高端CPU并支持多通道内存、足够的 PCI-E 插槽。  
- **内存**: 192GB（48GB × 4 DDR5）  
  - 为文档检索、特征向量处理、模型推理提供充足的内存空间。  
- **GPU**: RTX 5090 32GB  
  - 提供主力推理计算能力。32GB 显存有利于在 Ktransformer 模式下部署 Deepseek r1。  

在此配置下，可支持中大型规模的标书生成任务。如果后续业务量大幅增长，需要进一步升级 GPU 数量或显存容量，以满足更高并发或更大模型的推理需求。

---

## 6. 实施步骤

1. **环境搭建**  
   - 选择合适的操作系统（建议使用最新版本的 Ubuntu LTS 或 CentOS / Rocky Linux 等）。  
   - 安装 CUDA、PyTorch 等深度学习框架依赖，以及检索库（Faiss / Milvus / Elasticsearch 等）。  

2. **模型部署**  
   - 获取 Deepseek r1 模型权重，完成在本地服务器的安装与初始化。  
   - 根据Ktransformer的使用指引，配置推理脚本，使其能充分利用GPU和CPU的分块并行。  

3. **文档解析与索引**  
   - 搭建统一的文档入库流程，对输入的标书或其他资料进行格式解析（PDF、Word等）。  
   - 利用OCR或文本解析工具（如 Tesseract、PyPDF 等）提取文字。  
   - 将解析后的文本内容进行分段索引，存入向量数据库或全文检索引擎，为后续的 RAG 检索做好准备。  

4. **前端问题输入与检索**  
   - 搭建一个简单的前端页面或 CLI 界面，用户可以输入问题或需求。  
   - 后端根据输入调用检索服务（向量检索 / 关键词检索），获取与问题最相关的文档片段。  

5. **生成答案**  
   - 将检索结果（文本片段）与用户问题一起输入给本地部署的 LLM（Deepseek r1 + Ktransformer）。  
   - 生成答案或自动填充标书关键内容，输出完成的标书文档。  

6. **测试与验证**  
   - 通过一系列典型标书案例，验证系统在检索准确率、答案完整度、响应速度等方面的表现。  
   - 根据测试反馈，不断优化检索策略、RAG流程和模型推理流程。  

---

## 7. 后续扩展

1. **多模态支持**  
   - 后续若需要对图像、表格等内容进行解析与自动生成，可增加多模态模型组件或相应的预处理模块。  
2. **知识管理与版本控制**  
   - 为避免文档更新或版本冲突，需要在系统内加入文档及模型的版本管理策略。  
3. **自动化审核**  
   - 进一步使用问答检测或自然语言检查技术，对生成结果进行合规性、准确性检测，减少人工校对成本。  

---

*此设计文档仅为初始版本，旨在明确项目方向与实施方案。随着需求和技术的变化，可能需要进一步迭代和细化。*
